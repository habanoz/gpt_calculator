<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT Parameter Calculator</title>
    <script src="https://cdn.tailwindcss.com/3.4.1"></script>
</head>
<body class="bg-gray-100 min-h-screen flex items-center justify-center">
    <div class="bg-white p-8 rounded-lg shadow-md w-100">
        <h1 class="text-2xl font-bold mb-4">GPT Calculator</h1>
        <div class="mb-4">
            <label for="seq-length" class="block text-sm font-medium text-gray-700">Sequence Length:</label>
            <input type="number" value="1024" id="seq-length" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50" required>
        </div>
        <div class="mb-4">
            <label for="hidden-dim" class="block text-sm font-medium text-gray-700">Hidden Dimension:</label>
            <input type="number" value="768" id="hidden-dim" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50" required>
        </div>
        <div class="mb-4">
            <label for="num-heads" class="block text-sm font-medium text-gray-700">Number of Heads:</label>
            <input type="number" value="2" id="num-heads" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50" required>
        </div>
        <div class="mb-4">
            <label for="num-layers" class="block text-sm font-medium text-gray-700">Number of Layers:</label>
            <input type="number" value="12" id="num-layers" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50" required>
        </div>
        <div class="mb-4">
            <label for="vocab-size" class="block text-sm font-medium text-gray-700">Vocabulary Size:</label>
            <input type="number" value="50257" id="vocab-size" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50" required>
        </div>
        <div class="mb-4">
            <label for="bias" class="block text-sm font-medium text-gray-700">Include bias:</label>
            <input type="checkbox" id="bias" class="mt-1 block">
        </div> 
        <div class="mb-4">
            <label for="weightTying" class="block text-sm font-medium text-gray-700">Weight tying:</label>
            <input type="checkbox" id="weightTying" class="mt-1 block" checked>
        </div>
        <div class="mb-4">
            <label for="includeEmbeddings" class="block text-sm font-medium text-gray-700">Include Embeddings:</label>
            <input type="checkbox" id="includeEmbeddings" class="mt-1 block" checked>
        </div>
        
        <h2 class="text-2xl font-semibold mb-4">Training Estimations</h1>
        
        <div class="mb-4">
            <label for="tflops" class="block text-sm font-medium text-gray-700">GPU Teraflop per second(TFLOPS):</label>
            <input type="number" value="65" id="tflops" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50" required>
        </div>
        <div class="mb-4">
            <label for="mfu" class="block text-sm font-medium text-gray-700">Model FLOP Utilization (MFU):</label>
            <input type="number" value="0.22" id="mfu" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50" required>
        </div>

        <h2 class="text-2xl font-semibold mb-4">Memory Estimations</h1>
        
            <div class="mb-4">
                <label for="batch_size" class="block text-sm font-medium text-gray-700">Batch Size:</label>
                <input type="number" value="8" id="batch_size" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50" required>
            </div>
            <div class="mb-4">
                <label for="amp" class="block text-sm font-medium text-gray-700">Automatic Mixed Precision (AMP):</label>
                <input type="checkbox" id="amp" class="mt-1 block" checked>
            </div>          
            <div class="mb-4">
                <label for="dropout" class="block text-sm font-medium text-gray-700">Use Dropout:</label>
                <input type="checkbox" id="dropout" class="mt-1 block" checked>
            </div>

            <div class="mb-4">
                <label for="optimizerSelect" class="block text-sm font-medium text-gray-700">Optimizer:</label>
                <div class="relative inline-block w-full">
                    <select id="optimizerSelect" class="block appearance-none w-full bg-white border border-gray-400 hover:border-gray-500 px-4 py-2 pr-8 rounded shadow leading-tight focus:outline-none focus:shadow-outline">
                      <option value="adamw">AdamW</option>
                      <option value="sdg">SDG</option>
                      <option value="8bitadam">8_Bit Adam</option>
                    </select>
                    <div class="pointer-events-none absolute inset-y-0 right-0 flex items-center px-2 text-gray-700">
                      <svg class="fill-current h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z"/></svg>
                    </div>
                  </div>
                          
            </div>

        <button onclick="calculateParameters()" class="w-full bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded">
            Calculate
        </button>
        
        <div id="params" class="mt-4 text-lg"></div>
        <div id="estimations" class="mt-4 text-lg font-semibold" style="display: none;">Chinchilla (Approach 2) Estimations</div>
        <div id="tokens" class="mt-4 text-lg"></div>
        <div id="total_flops" class="mt-4 text-lg"></div>
        <div id="hours" class="mt-4 text-lg"></div>
        <div id="llama_estimations" class="mt-4 text-lg font-semibold" style="display: none;">Llama3 Estimations for the same FLOPs</div>
        <div id="llama_estimation_token" class="mt-4 text-lg"></div>
        <div id="llama_estimation_param" class="mt-4 text-lg"></div>
        <div id="memory_usage" class="mt-4 text-lg font-semibold" style="display: none;">GPU Memory Usage</div>
        <div id="total_memory_usage" class="mt-4 text-lg"></div>
        <div id="activation_memory_usage" class="mt-4 text-lg"></div>
        <div id="grad_memory_usage" class="mt-4 text-lg"></div>
        <div id="parameter_memory_usage" class="mt-4 text-lg"></div>
        <div id="optimizer_memory_usage" class="mt-4 text-lg"></div>
    </div>

    <script>
        function projectTokens(n_params){
            const m = 1.041;
            const c = 0.9354;
            return Math.pow(10, (m * Math.log10(n_params)) + c);
        }

        function llamaLaws(c){
            const a = 0.5372651710628614;
            const tokens = 0.299*(c**a);
            const params = c / (5.711*tokens);
            return [tokens, params];
        }

        function calcBlock(B, S, D, NH, amp = false, dropout = false, bias = false) {
            const bo = 1;
            const fp = 4;
            const mp = amp? 2 : 4;
            const hp = 2;
            const i64 = 8;

            let total = 0;

            // layer norm1
            total += B * S * D * fp // NativeLayerNormBackward0-input
            total += B * S * fp; // NativeLayerNormBackward0-result1
            total += B * S * fp; // NativeLayerNormBackward0-result2
            total += (bias? 0 : 0) * fp; // NativeLayerNormBackward0-bias

            // layer norm2
            total += B * S * D * fp; // NativeLayerNormBackward0-input
            total += B * S * fp; // NativeLayerNormBackward0-result1
            total += B * S * fp; // NativeLayerNormBackward0-result2
            total += (bias? 0 : 0) * fp; // NativeLayerNormBackward0-bias

            // scaled_dot_product_attention efficient implementation
            total += B * S * D * mp; // MmBackward0-self (c_attn)
            total += (amp? 3 * D * D * hp : 0); // MmBackward0-mat2 (c_attn)

            total += B * S * D * mp; // ScaledDotProductEfficientAttentionBackward0-value-query-key
            total += B * S * D * mp; // ScaledDotProductEfficientAttentionBackward0-value-query-key
            total += B * S * D * mp; // ScaledDotProductEfficientAttentionBackward0-value-query-key
            total += B * NH * S * fp; // ScaledDotProductEfficientAttentionBackward0-log_sumexp
            total += i64; // ScaledDotProductEfficientAttentionBackward0-philox_offset
            total += i64; // ScaledDotProductEfficientAttentionBackward0-philox_seed
            // this is accounted in c_proj matmul, avoid adding twice
            // total += B * S * D * mp; // ScaledDotProductEfficientAttentionBackward0-output

            // attention-c_proj
            total += (amp? D * D * hp : 0); // MmBackward0-mat2 (c_proj)
            total += B * S * D * mp // same as ScaledDotProductEfficientAttentionBackward0-output // MmBackward0-self  (c_proj)

            // attention-droput
            total += (dropout? B * S * D * 1 : 0); // NativeDropoutBackward0-result1

            // MLP
            total += (amp? S * D * hp : 0); // MmBackward0-mat2 (c_fc)
            total += B * S * D * mp; // MmBackward0-self (c_fc)

            total += 4 * B * S * D * mp; // GeluBackward0-self

            total += (amp? S * D * hp : 0); // MmBackward0-mat2 (c_proj)
            total += 4 * B * S * D * mp; // MmBackward0-self (c_proj), this tensor comes from gelu

            // mlp dropout
            total += (dropout? B * S * D * 1 : 0); // NativeDropoutBackward0-result1

            return total;
        }

        function calcGpt(B, S, D, NH, V, nLayers = 1, amp = false, dropout = false, bias = false) {
            const bo = 1;
            const fp = 4;
            const mp = amp? 2 : 4;
            const hp = 2;
            const i64 = 8;

            let total = 0;

            // token embeddings
            total += B * S * i64;

            // position embeddings
            total += S * i64;

            // x = self.transformer.drop(tok_emb + pos_emb)
            total += (dropout? B * S * D * bo : 0);

            // add blocks
            total += nLayers * calcBlock(B, S, D, NH, amp, dropout, bias);

            // final ln
            total += B * S * D * fp;
            total += B * S * fp;
            total += B * S * fp;
            total += (bias? 0 * fp : 0); // NativeLayerNormBackward0-bias

            // lm_head
            total += (amp? V * D * hp : 0);
            total += B * S * D * mp;

            return total;
        }

        function getOptimizerBytes(optimizer) {
            switch (optimizer) {
                case 'adamw':
                    return 8;
                case 'sdg':
                    return 4;
                case '8bitadam':
                    return 2;
                default:
                    throw new Error(`Unsupported optimizer: ${optimizer}`);
            }
        }

        function calculateParameters() {
            const sequenceLength = parseInt(document.getElementById('seq-length').value);
            const hiddenDim = parseInt(document.getElementById('hidden-dim').value);
            const numLayers = parseInt(document.getElementById('num-layers').value);
            const vocabSize = parseInt(document.getElementById('vocab-size').value);
            const includeBias = document.getElementById('bias').checked; 
            const weightTying = document.getElementById('weightTying').checked; 
            const includeEmbeddings = document.getElementById('includeEmbeddings').checked; 
            const mfu = parseFloat(document.getElementById('mfu').value);
            const tflops = parseFloat(document.getElementById('tflops').value);
            
            const numHeads = parseFloat(document.getElementById('num-heads').value);
            const batchSize = parseInt(document.getElementById('batch_size').value);
            const amp = document.getElementById('amp').checked;
            const dropout = document.getElementById('dropout').checked;
            const selectedOptimizer = document.getElementById('optimizerSelect').value;

            if (isNaN(sequenceLength) || isNaN(hiddenDim) || isNaN(numLayers) || isNaN(vocabSize)) {
                document.getElementById('result').textContent = 'Please enter valid numbers.';
                return;
            }

            if (!isNaN(mfu) && (mfu <0 || mfu > 1)){
                document.getElementById('result').textContent = 'MFU should be in range [0,1].';
                return;
            }

            // Calculate parameters
            const wte = vocabSize * hiddenDim; // Token embedding
            const wpe = hiddenDim * sequenceLength; // Positional embedding
            
            // Per layer
            const ln_1 = hiddenDim; // layer norm 1
            const attn_c_attn = 3 * hiddenDim * hiddenDim; // Query, Key, Value projections
            const attn_c_proj = hiddenDim * hiddenDim; // Output projection
            const ln_2 = hiddenDim; // layer norm 2
            const mlp_c_fc = 4 * hiddenDim * hiddenDim; // First MLP layer
            const mlp_c_proj = 4 * hiddenDim * hiddenDim; // Second MLP layer

            const paramsPerLayer = ln_1 + attn_c_attn + attn_c_proj + ln_1 + mlp_c_fc + mlp_c_proj;
            const final_ln = hiddenDim
            const final_dense = vocabSize * hiddenDim

            const totalParamsWOEmbedding = (paramsPerLayer * numLayers) + final_ln + final_dense; 
            let totalParams = totalParamsWOEmbedding;
            let totalParamsWEmbedding = totalParamsWOEmbedding;

            if (includeEmbeddings){
                totalParams += wpe
                if (!weightTying){
                    // if weight tying, wte and final_dense is shared. Only count wte when weight tying is off
                    totalParams += wte; 
                }
            }

            totalParamsWEmbedding += wpe;
            if (!weightTying){
                // if weight tying, wte and final_dense is shared. Only count wte when weight tying is off
                totalParamsWEmbedding += wte; 
            }

            if(includeBias){
                // each layer has 6 biases
                // - ln_1.bias
                // - c_attn.bias (3 * hidden dimension)
                // - c_proj.bias
                // - ln_2.bias
                // - c_fc.bias (4 * hidden dimension)
                // - c_proj.bias
                totalParams += 11 * numLayers * hiddenDim
                
                // final ln has a bias
                totalParams += hiddenDim
            }

            document.getElementById('params').textContent = `#parameters: ${totalParams.toLocaleString()}`;
            
            if (!isNaN(tflops) && !isNaN(mfu)){
                tokens = projectTokens(totalParamsWOEmbedding);
                total_compute = 6 * totalParamsWOEmbedding * tokens; // 6ND
                effective_flops = tflops * 1e12 * mfu;

                total_seconds = (total_compute / effective_flops);
                total_hours = total_seconds / 3600;

                const [ll_tokens, ll_params] = llamaLaws(total_compute);
                
                const parameterMemory = totalParamsWEmbedding * 4;
                const gradientMemory = totalParamsWEmbedding * 4;
                const optimizerMemory = totalParamsWEmbedding * getOptimizerBytes(selectedOptimizer);
                const activationMemory = calcGpt(batchSize, sequenceLength, hiddenDim, numHeads, vocabSize, numLayers, amp, dropout, includeBias);
                const totalMemory = activationMemory + optimizerMemory + parameterMemory+gradientMemory; 

                document.getElementById('estimations').style.display = `block`;
                document.getElementById('tokens').textContent = `Optimal Tokens: ${tokens.toExponential(3)}`;
                document.getElementById('total_flops').textContent = `Cost (FLOP): ${total_compute.toExponential(3)}`;
                document.getElementById('hours').textContent = `Duration (Hour): ${Math.round(total_hours)}`;
                document.getElementById('llama_estimations').style.display = `block`;
                document.getElementById('llama_estimation_token').textContent = `Optimal Tokens: ${ll_tokens.toExponential(3)}`;
                document.getElementById('llama_estimation_param').textContent = `Optimal Params: ${ll_params.toExponential(3)}`;

                document.getElementById('memory_usage').style.display = `block`;
                document.getElementById('total_memory_usage').textContent = `Total: ${totalMemory.toLocaleString()}`;
                document.getElementById('activation_memory_usage').textContent = `Activation: ${activationMemory.toLocaleString()}`;
                document.getElementById('grad_memory_usage').textContent = `Gradients: ${gradientMemory.toLocaleString()}`;
                document.getElementById('parameter_memory_usage').textContent = `Parameters: ${parameterMemory.toLocaleString()}`;
                document.getElementById('optimizer_memory_usage').textContent = `Optimizer: ${optimizerMemory.toLocaleString()}`;
            }else{
                document.getElementById('tokens').textContent = ``;
                document.getElementById('estimations').textContent = ``;
            }
        }
    </script>
</body>
</html>